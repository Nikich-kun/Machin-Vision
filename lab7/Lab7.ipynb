{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern recognition: Lab 7\n",
    "### Tasks:\n",
    "* Plot the error\n",
    "* Model XOR with the help of sigmoid\n",
    "* Add moments rule to learning equation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "k = 1\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-k*x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1.0-sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1.0 - x**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 0\n",
      "epochs: 10000\n",
      "epochs: 20000\n",
      "epochs: 30000\n",
      "epochs: 40000\n",
      "epochs: 50000\n",
      "epochs: 60000\n",
      "epochs: 70000\n",
      "epochs: 80000\n",
      "epochs: 90000\n",
      "[0 0] [-0.00057706]\n",
      "[0 1] [ 0.99555127]\n",
      "[1 0] [ 0.99678877]\n",
      "[1 1] [ 0.00185349]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.activation = tanh\n",
    "        self.activation_prime = tanh_prime\n",
    "\n",
    "        # Set weights\n",
    "        self.weights = []\n",
    "        # layers = [2,2,1]\n",
    "        # range of weight values (-1,1)\n",
    "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "        \n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "        # output layer - random((2+1, 1)) : 3 x 1\n",
    "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
    "        self.weights.append(r)\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=100000):\n",
    "        # Add column of ones to X\n",
    "        # This is to add the bias unit to the input layer\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "         \n",
    "        for k in range(epochs):\n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]]\n",
    "\n",
    "            for l in range(len(self.weights)):\n",
    "                    dot_value = np.dot(a[l], self.weights[l])\n",
    "                    activation = self.activation(dot_value)\n",
    "                    a.append(activation)\n",
    "            # output layer\n",
    "            error = y[i] - a[-1]\n",
    "            deltas = [error * self.activation_prime(a[-1])]\n",
    "\n",
    "            # we need to begin at the second to last layer \n",
    "            # (a layer before the output layer)\n",
    "            for l in range(len(a) - 2, 0, -1): \n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
    "\n",
    "            # reverse\n",
    "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "            # backpropagation\n",
    "            # 1. Multiply its output delta and input activation \n",
    "            #    to get the gradient of the weight.\n",
    "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                self.weights[i] += learning_rate * layer.T.dot(delta)\n",
    "\n",
    "            if k % 10000 == 0: \n",
    "                print('epochs:', k)\n",
    "\n",
    "    def predict(self, x): \n",
    "    \n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)))      \n",
    "\n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        return a\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    nn = NeuralNetwork([2,2,1])\n",
    "    X = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "    y = np.array([0, 1, 1, 0])\n",
    "#     X = np.array([[-1, -1],\n",
    "#                   [-1, 1],\n",
    "#                   [1, -1],\n",
    "#                   [1, 1]])\n",
    "#     y = np.array([0, 1, 1, 0])\n",
    "\n",
    "    nn.fit(X, y)\n",
    "    for e in X:\n",
    "        print(e,nn.predict(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 0\n",
      "epochs: 10000\n",
      "epochs: 20000\n",
      "epochs: 30000\n",
      "epochs: 40000\n",
      "epochs: 50000\n",
      "epochs: 60000\n",
      "epochs: 70000\n",
      "epochs: 80000\n",
      "epochs: 90000\n",
      "[0 0] [ 0.4651934]\n",
      "[0 1] [ 0.47936946]\n",
      "[1 0] [ 0.47660624]\n",
      "[1 1] [ 0.48655351]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDH1Jasj/AclrffpE/KMSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109bda128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "k = 1\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-k*x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1.0-sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1.0 - x**2\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        \n",
    "        # self.activation = tanh\n",
    "        # self.activation_prime = tanh_prime\n",
    "        self.activation = sigmoid\n",
    "        self.activation_prime = sigmoid_prime\n",
    "\n",
    "        # Set weights\n",
    "        self.weights = []\n",
    "        # layers = [2,2,1]\n",
    "        # range of weight values (-1,1)\n",
    "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "\n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "        # output layer - random((2+1, 1)) : 3 x 1\n",
    "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
    "        self.weights.append(r)\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=100000):\n",
    "        # Add column of ones to X\n",
    "        # This is to add the bias unit to the input layer\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "\n",
    "        for k in range(epochs):\n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]]\n",
    "\n",
    "            for l in range(len(self.weights)):\n",
    "                    dot_value = np.dot(a[l], self.weights[l])\n",
    "                    activation = self.activation(dot_value)\n",
    "                    a.append(activation)\n",
    "            # output layer\n",
    "            error = y[i] - a[-1]\n",
    "            deltas = [error * self.activation_prime(a[-1])]\n",
    "\n",
    "            # we need to begin at the second to last layer \n",
    "            # (a layer before the output layer)\n",
    "            for l in range(len(a) - 2, 0, -1): \n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
    "\n",
    "            # reverse\n",
    "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "            # backpropagation\n",
    "            # 1. Multiply its output delta and input activation \n",
    "            #    to get the gradient of the weight.\n",
    "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                self.weights[i] += learning_rate * layer.T.dot(delta)\n",
    "\n",
    "            if k % 10000 == 0: \n",
    "                print('epochs:', k)\n",
    "\n",
    "    def predict(self, x): \n",
    "\n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)))      \n",
    "\n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        return a\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    nn = NeuralNetwork([2,2,1])\n",
    "    X = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "    y = np.array([0, 1, 1, 0])\n",
    "#     X = np.array([[-1, -1],\n",
    "#                   [-1, 1],\n",
    "#                   [1, -1],\n",
    "#                   [1, 1]])\n",
    "#     y = np.array([0, 1, 1, 0])\n",
    "\n",
    "    nn.fit(X, y)\n",
    "    for e in X:\n",
    "        baboom = nn.predict(e)\n",
    "        print(e,nn.predict(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
